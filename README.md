# DeepLOB-Style CNN+LSTM on Binance BTCUSDT LOB (1-Second Snapshots)

This project implements a DeepLOB-style CNN+LSTM model in PyTorch to predict
short-term mid-price movements (Up / Down) from 1-second Binance
Futures BTCUSDT limit order book snapshots.

The pipeline is structured as ordered scripts in the `steps/` directory,
so you can clearly see and run each step in sequence.

---

## Project Structure

```text
my_deeplob_project/
├── steps/                        # Executable pipeline scripts (run in order)
│   ├── S0_filter_raw_data.py     # Optional: filter raw data to February-only
│   ├── S1_download_data.py       # Inspect / fetch raw LOB files
│   ├── S2_preprocess_lob.py      # Clean + align raw LOB to 1s grid
│   ├── S2b_check_data_quality.py # Optional: quick sanity checks on preprocessing
│   ├── S3_build_sequences_and_labels.py
│   ├── S4_train_model.py
│   └── S5_evaluate_model.py
│
├── lob/                          # DeepLOB-specific modules (imported only)
│   ├── __init__.py
│   ├── model_deeplob.py          # DeepLOB model architecture
│   └── dataset_sharded.py        # Sharded dataset implementation
│
├── config.py                     # Central configuration (paths, hyperparameters)
├── utils.py                      # Common utilities (logging, seeding, etc.)
├── README.md
├── QUICKSTART.md
├── requirements.txt
│
├── data/
│   ├── raw/                      # Put raw BTCUSDT LOB files here
│   ├── preprocessed/             # Generated by S2_preprocess_lob.py
│   └── sequences/                # Generated by S3_build_sequences_and_labels.py
│
├── models/                       # Saved models (from S4_train_model.py)
├── logs/                         # Training logs (from S4_train_model.py)
└── results/                      # Evaluation outputs (from S5_evaluate_model.py)
```

### Important Conventions

- **Scripts with prefix `S`** (in `steps/`): These are executable pipeline steps that you run manually in sequence.
- **Modules without `S` prefix** (in `lob/`, `config.py`, `utils.py`): These are imported by other files and should NOT be executed directly.

---

## Installation

Create and activate a virtual environment (recommended):

```bash
python -m venv venv
source venv/bin/activate   # on Linux / macOS
# or:
venv\Scripts\activate      # on Windows
```

Install requirements:

```bash
pip install -r requirements.txt
```

---

## Data Expectations

Raw Binance BTCUSDT Futures LOB data should be placed in:

```
data/raw/
```

Example file names:

```
data/raw/btcusdt_depth_2024-02-01.parquet
data/raw/btcusdt_depth_2024-02-02.parquet
...
```

Or CSV files:

```
data/raw/BTCUSDT_lob_1s_10levels_year.csv
```

Each file is expected to contain:

- A timestamp column (Unix ms or parsable string).

For each level i = 1..N (N = config.data_config.num_levels, default 10):

- `bid_px_i`, `bid_qty_i`, `ask_px_i`, `ask_qty_i` (for existing Binance data)
- OR `bid_price_i`, `bid_size_i`, `ask_price_i`, `ask_size_i` (alternative format)

You can adjust paths, symbol, number of levels, etc. in `config.py`.

---

## Pipeline: Order of Execution

**Important**: Always run scripts from the project root directory.

Run these scripts in order:

### Step 0 (optional): Filter raw data to February 2024

```bash
python steps/S0_filter_raw_data.py
```

Filters raw LOB data to February 2024 only. Useful if you have full-year data but want to work with a smaller subset.

### Step 1: Download/Verify Raw Data

```bash
python steps/S1_download_data.py
```

Defines expected file format and location. Use it to inspect sample raw files and verify columns.

(Optional) extend to include real Binance download logic.

### Step 2: Preprocess LOB Data

```bash
python steps/S2_preprocess_lob.py
```

Loads raw LOB files from `data/raw/`. Aligns snapshots to a strict 1-second grid. Enforces a fixed number of levels per side. Saves standardized LOB to `data/preprocessed/lob_preprocessed_btcusdt.npz`.

**Optional verification**:

```bash
python steps/S2b_check_data_quality.py
```

Quick check to verify preprocessed LOB data size and quality.

### Step 3: Build Sequences and Labels

```bash
python steps/S3_build_sequences_and_labels.py
```

Loads preprocessed LOB. Computes mid-prices from best bid/ask. Builds sequences of length L (`config.label_config.seq_len`). Computes binary Up/Down labels using prediction horizon k (`config.label_config.pred_horizon`) and alpha quantile filtering. Splits into train/val/test (by time) and saves sharded sequences to `data/sequences/`.

### Step 4: Train Model

```bash
python steps/S4_train_model.py
```

Loads train/val sequences from sharded files. Trains DeepLOB model with Adam optimizer, CrossEntropyLoss, optional class weights. Applies early stopping on validation loss. Saves the best model to `models/deeplob_btcusdt.pt` and training logs to `logs/`.

**Note**: The model architecture is defined in `lob/model_deeplob.py` (imported automatically).

### Step 5: Evaluate Model

```bash
python steps/S5_evaluate_model.py
```

Loads test sequences and labels. Loads the best saved model. Computes accuracy, per-class precision/recall/F1, and confusion matrix. Prints metrics and saves them to `results/evaluation_results.json`.

---

## Quick Start with Existing Data

If you already have Binance LOB CSV files (like `BTCUSDT_lob_1s_10levels_year.csv`):

1. Copy your CSV file(s) to `data/raw/`
2. Run the pipeline:
   ```bash
   # Optional if you want February-only subset
   python steps/S0_filter_raw_data.py

   python steps/S1_download_data.py                # Verify data format
   python steps/S2_preprocess_lob.py               # Preprocess LOB
   python steps/S2b_check_data_quality.py          # Optional: verify preprocessing
   python steps/S3_build_sequences_and_labels.py   # Build sequences (sharded)
   python steps/S4_train_model.py                  # Train model
   python steps/S5_evaluate_model.py               # Evaluate model
   ```

---

## Configuration

All paths and hyperparameters are defined in `config.py`:

- **Data paths**: `paths` (raw, preprocessed, sequences, models, logs, results)
- **LOB/data settings**: `data_config` (symbol, num_levels, resample frequency, column_naming)
- **Sequence & label settings**: `label_config` (seq_len, pred_horizon, alpha_quantile)
- **Training settings**: `training_config` (batch_size, lr, epochs, early stopping)
- **Model settings**: `model_config` (num_classes, num_features)

Edit `config.py` to match your environment and experimental choices.

---

## Module Structure

### `lob/` - DeepLOB-Specific Modules

These modules are imported by the pipeline scripts but not executed directly:

- **`lob/model_deeplob.py`**: Defines the DeepLOB CNN+LSTM model architecture. Imported by `steps/S4_train_model.py` and `steps/S5_evaluate_model.py`.
- **`lob/dataset_sharded.py`**: Implements `ShardedSequenceDataset` for loading sequences from multiple shard files without loading everything into memory.

### Root-Level Modules

- **`config.py`**: Central configuration file containing all paths, hyperparameters, and settings.
- **`utils.py`**: Common utilities (logging, seeding, device detection, directory creation).

---

## Notes

- The pipeline is designed to be modular and readable.
- You can extend it with:
  - Additional features (imbalance, spread, volatility).
  - Alternative label schemes (e.g., smoothed mid-price).
  - More detailed logging/plotting (e.g. training curves).
- All scripts assume Bitcoin (BTCUSDT perpetual) only, as specified.
- The code supports both `px_qty` and `price_size` column naming conventions.
- Training uses sharded datasets to minimize memory usage (only one shard loaded at a time).

---

## Backup

A backup of the original project structure (before refactoring) is available in:

```
backup_before_refactor_20251118/
```

This contains all scripts and modules as they were before the refactoring.
